# LLM_Evaluation

### General Knowledge
- MMLU leaderboard : tests knowledge across 57 subjects, from science to humanities. Multi choices are given for a question and the model should find the right answer. Here is the leaderboard for MMLU-pro (increasing to 10 options rather than 4 with original MMLU,  integrates more challenging, reasoning-focused questions): https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro
### Reasoning Benchmarks
- BBH tests logical thinking and planning (using multi options answer)
- GSM8K specifically targets mathematical problem-solving.
### Reasoning Benchmarks
- HELM provides a holistic evaluation framework. it includes eval datasets from MMLU, GPQA, IFEVAL, ...
### Domain-Specific Benchmarks
- The Math Benchmark
- The HumanEval Benchmark is a coding-focused evaluation dataset
### Evaluation Arenas
-  Chatbot Arena offer a unique approach to LLM assessment through crowdsourced feedback.


Open LLM Leaderboard for ranking opensource models : https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?official=true
